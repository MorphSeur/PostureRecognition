{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25724fc-0207-4193-b62e-3163be78f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame by Frame\n",
    "import cv2\n",
    "vidcap = cv2.VideoCapture(\"input/YDXJ0098.mp4\")\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "    cv2.imwrite(\"input/frames/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "    print('Read a new frame: ', success)\n",
    "    count += 1\n",
    "    print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c185384-0dad-482f-9f2f-d90d0d6eaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame Recognition\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from Detection.Utils import ResizePadding\n",
    "from CameraLoader import CamLoader, CamLoader_Q\n",
    "from DetectorLoader import TinyYOLOv3_onecls\n",
    "\n",
    "from PoseEstimateLoader import SPPE_FastPose\n",
    "from fn import draw_single\n",
    "\n",
    "from Track.Tracker import Detection, Tracker\n",
    "from ActionsEstLoader import TSSTG\n",
    "\n",
    "def preproc(image):\n",
    "    \"\"\"preprocess function for CameraLoader.\n",
    "    \"\"\"\n",
    "    image = resize_fn(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def kpt2bbox(kpt, ex=20):\n",
    "    \"\"\"Get bbox that hold on all of the keypoints (x,y)\n",
    "    kpt: array of shape `(N, 2)`,\n",
    "    ex: (int) expand bounding box,\n",
    "    \"\"\"\n",
    "    return np.array((kpt[:, 0].min() - ex, kpt[:, 1].min() - ex,\n",
    "                     kpt[:, 0].max() + ex, kpt[:, 1].max() + ex))\n",
    "\n",
    "\n",
    "source = './input/2YDXJ0101.mp4'\n",
    "camera = source\n",
    "detection_input_size = 1920\n",
    "pose_input_size = '224x160'\n",
    "pose_backbone = 'resnet50'\n",
    "show_detected = True\n",
    "show_skeleton = True\n",
    "save_out = \"./output/out271220212YDXJ0101.avi\"\n",
    "device = 'cuda'\n",
    "\n",
    "# DETECTION MODEL.\n",
    "inp_dets = detection_input_size\n",
    "detect_model = TinyYOLOv3_onecls(inp_dets, device=device)\n",
    "\n",
    "# POSE MODEL.\n",
    "inp_pose = pose_input_size.split('x')\n",
    "inp_pose = (int(inp_pose[0]), int(inp_pose[1]))\n",
    "pose_model = SPPE_FastPose(pose_backbone, inp_pose[0], inp_pose[1], device=device)\n",
    "\n",
    "# Tracker.\n",
    "max_age = 30\n",
    "tracker = Tracker(max_age=max_age, n_init=3)\n",
    "\n",
    "# Actions Estimate.\n",
    "action_model = TSSTG()\n",
    "\n",
    "resize_fn = ResizePadding(inp_dets, inp_dets)\n",
    "\n",
    "# cam_source = camera\n",
    "# if type(cam_source) is str and os.path.isfile(cam_source):\n",
    "    # Use loader thread with Q for video file.\n",
    "    # cam = CamLoader_Q(cam_source, queue_size=100000, preprocess=preproc).start()\n",
    "    # print (\"Maybe camera\")\n",
    "# else:\n",
    "    # Use normal thread loader for webcam.\n",
    "    # cam = CamLoader(int(cam_source) if cam_source.isdigit() else cam_source,\n",
    "                    # preprocess=preproc).start()\n",
    "    # print (\"Maybe video\")\n",
    "\n",
    "#frame_size = cam.frame_size\n",
    "#scf = torch.min(inp_size / torch.FloatTensor([frame_size]), 1)[0]\n",
    "\n",
    "# outvid = False\n",
    "outvid = True\n",
    "codec = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "writer = cv2.VideoWriter(save_out, codec, 30, (inp_dets * 2, inp_dets * 2))\n",
    "\n",
    "fps_time = 0\n",
    "f = 0\n",
    "for x in range(1998):\n",
    "    path = \"./input/frames/frame\" + str(x) + \".jpg\"\n",
    "    print(path)\n",
    "    frame = cv2.imread(path)\n",
    "    frame = preproc(frame)\n",
    "    print (frame.shape)\n",
    "    # frame = cv2.resize(frame, (384, 384))\n",
    "    # print (frame.shape)\n",
    "    # image = frame.copy()\n",
    "\n",
    "    # Detect humans bbox in the frame with detector model.\n",
    "    detected = detect_model.detect(frame, need_resize=False, expand_bb=10)\n",
    "\n",
    "    # Predict each tracks bbox of current frame from previous frames information with Kalman filter.\n",
    "    tracker.predict()\n",
    "    # Merge two source of predicted bbox together.\n",
    "    for track in tracker.tracks:\n",
    "        det = torch.tensor([track.to_tlbr().tolist() + [0.5, 1.0, 0.0]], dtype=torch.float32)\n",
    "        detected = torch.cat([detected, det], dim=0) if detected is not None else det\n",
    "\n",
    "    detections = []  # List of Detections object for tracking.\n",
    "    if detected is not None:\n",
    "        #detected = non_max_suppression(detected[None, :], 0.45, 0.2)[0]\n",
    "        # Predict skeleton pose of each bboxs.\n",
    "        poses = pose_model.predict(frame, detected[:, 0:4], detected[:, 4])\n",
    "\n",
    "        # Create Detections object.\n",
    "        detections = [Detection(kpt2bbox(ps['keypoints'].numpy()),\n",
    "                                np.concatenate((ps['keypoints'].numpy(),\n",
    "                                                ps['kp_score'].numpy()), axis=1),\n",
    "                                ps['kp_score'].mean().numpy()) for ps in poses]\n",
    "\n",
    "        # VISUALIZE.\n",
    "        # if show_detected:\n",
    "            # for bb in detected[:, 0:5]:\n",
    "                # frame = cv2.rectangle(frame, (bb[0], bb[1]), (bb[2], bb[3]), (0, 0, 255), 1)\n",
    "\n",
    "    # Update tracks by matching each track information of current and previous frame or\n",
    "    # create a new track if no matched.\n",
    "    tracker.update(detections)\n",
    "\n",
    "    # Predict Actions of each track.\n",
    "    for i, track in enumerate(tracker.tracks):\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_tlbr().astype(int)\n",
    "        center = track.get_center().astype(int)\n",
    "\n",
    "        action = 'pending..'\n",
    "        clr = (0, 255, 0)\n",
    "        # Use 30 frames time-steps to prediction.\n",
    "        if len(track.keypoints_list) == 30:\n",
    "            pts = np.array(track.keypoints_list, dtype=np.float32)\n",
    "            out = action_model.predict(pts, frame.shape[:2])\n",
    "            action_name = action_model.class_names[out[0].argmax()]\n",
    "            \n",
    "            img = cv2.imread(path)\n",
    "            name = str(action_name) + \" : \" + str(out[0].max() * 100)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(img, name, (15, 15), font, 0.5, (255, 255, 255), 1)\n",
    "            r = cv2.imwrite(\"./input/framesRec/frameRec2\" + str(x) + \".jpg\", img)\n",
    "            print(\"./input/framesRec/frameRec2\" + str(x) + \".jpg\")\n",
    "            print(\"is stored\")\n",
    "            \n",
    "            \n",
    "            # action = '{}: {:.2f}%'.format(action_name, out[0].max() * 100)\n",
    "            # if action_name == 'Fall Down':\n",
    "                # clr = (0, 255, 0)\n",
    "            # elif action_name == 'Lying Down':\n",
    "                # clr = (0, 255, 0)\n",
    "\n",
    "        # VISUALIZE.\n",
    "        # if track.time_since_update == 0:\n",
    "            # if show_skeleton:\n",
    "                # frame = draw_single(frame, track.keypoints_list[-1])\n",
    "            # frame = cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 1)\n",
    "            # frame = cv2.putText(frame, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, (255, 0, 0), 2)\n",
    "            # frame = cv2.putText(frame, action, (bbox[0] + 15, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, clr, 1)\n",
    "            # frame = cv2.putText(frame, action, (0, 95), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, clr, 1)\n",
    "\n",
    "    # Show Frame.\n",
    "    # frame = cv2.resize(frame, (0, 0), fx=2., fy=2.)\n",
    "    # frame = cv2.putText(frame, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),\n",
    "                        # (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    # frame = frame[:, :, ::-1]\n",
    "    # fps_time = time.time()\n",
    "\n",
    "    # if outvid:\n",
    "        # writer.write(frame)\n",
    "\n",
    "    # cv2.imshow('frame', frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # break\n",
    "\n",
    "# Clear resource.\n",
    "# cam.stop()\n",
    "# if outvid:\n",
    "    # writer.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd74c3-45df-4da9-8dfb-1b278d65e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frames Building\n",
    "import cv2\n",
    "\n",
    "img_array = []\n",
    "\n",
    "def toCheck(image):\n",
    "    x = 0\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "def openn(x):\n",
    "    path = \"./input/framesRec/frameRec\" + str(x) + \".jpg\"\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "    \n",
    "for x in range(1998):\n",
    "    f = openn(x)\n",
    "    if f is None:\n",
    "        x += 1\n",
    "        print (x)\n",
    "        toCheck(f)\n",
    "    else:   \n",
    "        height, width, layers = f.shape\n",
    "        size = (width, height)\n",
    "        img_array.append(f)\n",
    "\n",
    "        out = cv2.VideoWriter('./output/RecognizedFrames.avi', cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bd67a-b228-4341-bb68-b2443dcd68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame Recognition\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from Detection.Utils import ResizePadding\n",
    "from CameraLoader import CamLoader, CamLoader_Q\n",
    "from DetectorLoader import TinyYOLOv3_onecls\n",
    "\n",
    "from PoseEstimateLoader import SPPE_FastPose\n",
    "from fn import draw_single\n",
    "\n",
    "from Track.Tracker import Detection, Tracker\n",
    "from ActionsEstLoader import TSSTG\n",
    "\n",
    "def preproc(image):\n",
    "    \"\"\"preprocess function for CameraLoader.\n",
    "    \"\"\"\n",
    "    image = resize_fn(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def kpt2bbox(kpt, ex=20):\n",
    "    \"\"\"Get bbox that hold on all of the keypoints (x,y)\n",
    "    kpt: array of shape `(N, 2)`,\n",
    "    ex: (int) expand bounding box,\n",
    "    \"\"\"\n",
    "    return np.array((kpt[:, 0].min() - ex, kpt[:, 1].min() - ex,\n",
    "                     kpt[:, 0].max() + ex, kpt[:, 1].max() + ex))\n",
    "\n",
    "\n",
    "source = './input/2YDXJ0101.mp4'\n",
    "camera = source\n",
    "detection_input_size = 1920\n",
    "pose_input_size = '224x160'\n",
    "pose_backbone = 'resnet50'\n",
    "show_detected = True\n",
    "show_skeleton = True\n",
    "save_out = \"./output/out271220212YDXJ0101.avi\"\n",
    "device = 'cuda'\n",
    "\n",
    "# DETECTION MODEL.\n",
    "inp_dets = detection_input_size\n",
    "detect_model = TinyYOLOv3_onecls(inp_dets, device=device)\n",
    "\n",
    "# POSE MODEL.\n",
    "inp_pose = pose_input_size.split('x')\n",
    "inp_pose = (int(inp_pose[0]), int(inp_pose[1]))\n",
    "pose_model = SPPE_FastPose(pose_backbone, inp_pose[0], inp_pose[1], device=device)\n",
    "\n",
    "# Tracker.\n",
    "max_age = 30\n",
    "tracker = Tracker(max_age=max_age, n_init=3)\n",
    "\n",
    "# Actions Estimate.\n",
    "action_model = TSSTG()\n",
    "\n",
    "resize_fn = ResizePadding(inp_dets, inp_dets)\n",
    "\n",
    "# cam_source = camera\n",
    "# if type(cam_source) is str and os.path.isfile(cam_source):\n",
    "    # Use loader thread with Q for video file.\n",
    "    # cam = CamLoader_Q(cam_source, queue_size=100000, preprocess=preproc).start()\n",
    "    # print (\"Maybe camera\")\n",
    "# else:\n",
    "    # Use normal thread loader for webcam.\n",
    "    # cam = CamLoader(int(cam_source) if cam_source.isdigit() else cam_source,\n",
    "                    # preprocess=preproc).start()\n",
    "    # print (\"Maybe video\")\n",
    "\n",
    "#frame_size = cam.frame_size\n",
    "#scf = torch.min(inp_size / torch.FloatTensor([frame_size]), 1)[0]\n",
    "\n",
    "# outvid = False\n",
    "outvid = True\n",
    "codec = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "writer = cv2.VideoWriter(save_out, codec, 30, (inp_dets * 2, inp_dets * 2))\n",
    "\n",
    "fps_time = 0\n",
    "f = 0\n",
    "for x in range(1998):\n",
    "    path = \"./input/frames/frame\" + str(x) + \".jpg\"\n",
    "    print(path)\n",
    "    frame = cv2.imread(path)\n",
    "    frame = preproc(frame)\n",
    "    print (frame.shape)\n",
    "    # frame = cv2.resize(frame, (384, 384))\n",
    "    # print (frame.shape)\n",
    "    # image = frame.copy()\n",
    "\n",
    "    # Detect humans bbox in the frame with detector model.\n",
    "    detected = detect_model.detect(frame, need_resize=False, expand_bb=10)\n",
    "\n",
    "    # Predict each tracks bbox of current frame from previous frames information with Kalman filter.\n",
    "    tracker.predict()\n",
    "    # Merge two source of predicted bbox together.\n",
    "    for track in tracker.tracks:\n",
    "        det = torch.tensor([track.to_tlbr().tolist() + [0.5, 1.0, 0.0]], dtype=torch.float32)\n",
    "        detected = torch.cat([detected, det], dim=0) if detected is not None else det\n",
    "\n",
    "    detections = []  # List of Detections object for tracking.\n",
    "    if detected is not None:\n",
    "        #detected = non_max_suppression(detected[None, :], 0.45, 0.2)[0]\n",
    "        # Predict skeleton pose of each bboxs.\n",
    "        poses = pose_model.predict(frame, detected[:, 0:4], detected[:, 4])\n",
    "\n",
    "        # Create Detections object.\n",
    "        detections = [Detection(kpt2bbox(ps['keypoints'].numpy()),\n",
    "                                np.concatenate((ps['keypoints'].numpy(),\n",
    "                                                ps['kp_score'].numpy()), axis=1),\n",
    "                                ps['kp_score'].mean().numpy()) for ps in poses]\n",
    "\n",
    "        # VISUALIZE.\n",
    "        # if show_detected:\n",
    "            # for bb in detected[:, 0:5]:\n",
    "                # frame = cv2.rectangle(frame, (bb[0], bb[1]), (bb[2], bb[3]), (0, 0, 255), 1)\n",
    "\n",
    "    # Update tracks by matching each track information of current and previous frame or\n",
    "    # create a new track if no matched.\n",
    "    tracker.update(detections)\n",
    "\n",
    "    # Predict Actions of each track.\n",
    "    for i, track in enumerate(tracker.tracks):\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_tlbr().astype(int)\n",
    "        center = track.get_center().astype(int)\n",
    "\n",
    "        action = 'pending..'\n",
    "        clr = (0, 255, 0)\n",
    "        # Use 30 frames time-steps to prediction.\n",
    "        if len(track.keypoints_list) == 30:\n",
    "            pts = np.array(track.keypoints_list, dtype=np.float32)\n",
    "            out = action_model.predict(pts, frame.shape[:2])\n",
    "            action_name = action_model.class_names[out[0].argmax()]\n",
    "            \n",
    "            img = cv2.imread(path)\n",
    "            print(img.shape)\n",
    "            img = cv2.resize(img, (1920, 1920))\n",
    "            print(img.shape)\n",
    "            # img = draw_single(img, track.keypoints_list[-1])\n",
    "            # print('1111111111111111')\n",
    "            name = str(action_name) + \" : \" + str(out[0].max() * 100)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(img, name, (15, 15), font, 0.5, (255, 255, 255), 1)\n",
    "            r = cv2.imwrite(\"./input/framesRec2/frameRec\" + str(x) + \".jpg\", img)\n",
    "            print(\"./input/framesRec2/frameRec\" + str(x) + \".jpg\")\n",
    "            print(\"is stored\")\n",
    "            \n",
    "            \n",
    "            # action = '{}: {:.2f}%'.format(action_name, out[0].max() * 100)\n",
    "            # if action_name == 'Fall Down':\n",
    "                # clr = (0, 255, 0)\n",
    "            # elif action_name == 'Lying Down':\n",
    "                # clr = (0, 255, 0)\n",
    "\n",
    "        # VISUALIZE.\n",
    "        # if track.time_since_update == 0:\n",
    "            # if show_skeleton:\n",
    "                # frame = draw_single(frame, track.keypoints_list[-1])\n",
    "                # print('1111111111111111')\n",
    "            # frame = cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 1)\n",
    "            # frame = cv2.putText(frame, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, (255, 0, 0), 2)\n",
    "            # frame = cv2.putText(frame, action, (bbox[0] + 15, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, clr, 1)\n",
    "            # frame = cv2.putText(frame, action, (0, 95), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                # 0.4, clr, 1)\n",
    "\n",
    "    # Show Frame.\n",
    "    # frame = cv2.resize(frame, (0, 0), fx=2., fy=2.)\n",
    "    # frame = cv2.putText(frame, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),\n",
    "                        # (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    # frame = frame[:, :, ::-1]\n",
    "    # fps_time = time.time()\n",
    "\n",
    "    # if outvid:\n",
    "        # writer.write(frame)\n",
    "\n",
    "    # cv2.imshow('frame', frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # break\n",
    "\n",
    "# Clear resource.\n",
    "# cam.stop()\n",
    "# if outvid:\n",
    "    # writer.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272e4c6-b616-444c-a760-53bd1282779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame Recognition\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from Detection.Utils import ResizePadding\n",
    "from CameraLoader import CamLoader, CamLoader_Q\n",
    "from DetectorLoader import TinyYOLOv3_onecls\n",
    "\n",
    "from PoseEstimateLoader import SPPE_FastPose\n",
    "from fn import draw_single\n",
    "\n",
    "from Track.Tracker import Detection, Tracker\n",
    "from ActionsEstLoader import TSSTG\n",
    "\n",
    "def preproc(image):\n",
    "    image = resize_fn(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "\n",
    "def kpt2bbox(kpt, ex=20):\n",
    "    return np.array((kpt[:, 0].min() - ex, kpt[:, 1].min() - ex,\n",
    "                     kpt[:, 0].max() + ex, kpt[:, 1].max() + ex))\n",
    "\n",
    "\n",
    "source = './input/YDXJ0098.mp4'\n",
    "detection_input_size = 1920\n",
    "pose_input_size = '224x160'\n",
    "pose_backbone = 'resnet50'\n",
    "save_out = \"./output/out28122021YDXJ0098.avi\"\n",
    "device = 'cuda'\n",
    "\n",
    "# DETECTION MODEL.\n",
    "inp_dets = detection_input_size\n",
    "detect_model = TinyYOLOv3_onecls(inp_dets, device=device)\n",
    "\n",
    "# POSE MODEL.\n",
    "inp_pose = pose_input_size.split('x')\n",
    "inp_pose = (int(inp_pose[0]), int(inp_pose[1]))\n",
    "pose_model = SPPE_FastPose(pose_backbone, inp_pose[0], inp_pose[1], device=device)\n",
    "\n",
    "# Tracker.\n",
    "max_age = 30\n",
    "tracker = Tracker(max_age=max_age, n_init=3)\n",
    "\n",
    "# Actions Estimate.\n",
    "action_model = TSSTG()\n",
    "\n",
    "resize_fn = ResizePadding(inp_dets, inp_dets)\n",
    "\n",
    "# outvid = False\n",
    "outvid = True\n",
    "codec = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "writer = cv2.VideoWriter(save_out, codec, 30, (inp_dets * 2, inp_dets * 2))\n",
    "\n",
    "fps_time = 0\n",
    "f = 0\n",
    "for x in range(1998):\n",
    "    path = \"./input/frames/frame\" + str(x) + \".jpg\"\n",
    "    # print(path)\n",
    "    frame = cv2.imread(path)\n",
    "    frame = preproc(frame)\n",
    "    print (frame.shape)\n",
    "\n",
    "    # Detect humans bbox in the frame with detector model.\n",
    "    detected = detect_model.detect(frame, need_resize=False, expand_bb=10)\n",
    "\n",
    "    # Predict each tracks bbox of current frame from previous frames information with Kalman filter.\n",
    "    tracker.predict()\n",
    "    # Merge two source of predicted bbox together.\n",
    "    for track in tracker.tracks:\n",
    "        det = torch.tensor([track.to_tlbr().tolist() + [0.5, 1.0, 0.0]], dtype=torch.float32)\n",
    "        detected = torch.cat([detected, det], dim=0) if detected is not None else det\n",
    "\n",
    "    detections = []  # List of Detections object for tracking.\n",
    "    if detected is not None:\n",
    "        # Predict skeleton pose of each bboxs.\n",
    "        poses = pose_model.predict(frame, detected[:, 0:4], detected[:, 4])\n",
    "\n",
    "        # Create Detections object.\n",
    "        detections = [Detection(kpt2bbox(ps['keypoints'].numpy()),\n",
    "                                np.concatenate((ps['keypoints'].numpy(),\n",
    "                                                ps['kp_score'].numpy()), axis=1),\n",
    "                                ps['kp_score'].mean().numpy()) for ps in poses]\n",
    "    tracker.update(detections)\n",
    "\n",
    "    # Predict Actions of each track.\n",
    "    for i, track in enumerate(tracker.tracks):\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_tlbr().astype(int)\n",
    "        center = track.get_center().astype(int)\n",
    "\n",
    "        action = 'pending..'\n",
    "        clr = (0, 255, 0)\n",
    "        # Use 30 frames time-steps to prediction.\n",
    "        if len(track.keypoints_list) == 30:\n",
    "            pts = np.array(track.keypoints_list, dtype=np.float32)\n",
    "            out = action_model.predict(pts, frame.shape[:2])\n",
    "            action_name = action_model.class_names[out[0].argmax()]\n",
    "            \n",
    "            img = cv2.imread(path)\n",
    "            print(img.shape)\n",
    "            # img = cv2.resize(img, (1920, 1920))\n",
    "            # print(img.shape)\n",
    "            # img = draw_single(img, track.keypoints_list[-1])\n",
    "            # print('1111111111111111')\n",
    "            name = str(action_name) + \" : \" + str(out[0].max() * 100)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(img, name, (15, 15), font, 0.5, (255, 255, 255), 1)\n",
    "            r = cv2.imwrite(\"./input/framesRec2/frameRec\" + str(x) + \".jpg\", img)\n",
    "            print(\"./input/framesRec2/frameRec\" + str(x) + \".jpg\")\n",
    "            print(\"is stored\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
